---
title: "Expression analysis with R and the tidyverse"
output: 
  html_notebook:
    theme: readable
    highlight: tango
    number_sections: true
    toc: true
    toc_depth: 3
    toc_float: true

---
<style>
#header .btn-group {
    display: none;
}


/* Add a custom CSS counter for figures */
body {
  counter-reset: figure;
}
figure {
  counter-increment: figure;
}
figcaption:before {
  content: "Figure " counter(figure) " ";
  font-weight: bold;
}

/* Modify the style of text in figure captions */
figcaption {
  font-style: italic;
  color: #333;
  font-size: 0.9em;
}
</style>



<!-- ###################################################################### -->
<!-- ###################################################################### -->
# Intro and Overview
<!-- ###################################################################### -->
<!-- ###################################################################### -->

<!-- ###################################################################### -->
## Pre-requisites
<!-- ###################################################################### -->

### R and RStudio
<div style="text-align: justify;">
Make sure that **R** and **RStudio** are installed in your computer. You can create a new **R Script** in **RStudio** to copy example source code from this tutorial and write code for the exercises. Save the script in your home directory (or a sub directory of your choice). To run the code written on the current line (or of selected lines), click on button **Run** or press **CTRL+ENTER**. Results will be displayed either in the **RStudio**'s console or the Viewer panel (graphics). <br>
<span style="color: #336699; font-weight: bold;">To read the documentation about a function (Help panel), in the script, place the cursor on the function's name and press F1. You should use this feature extensively here, as many answers to possible questions are hidden here :)</span>

### package installation
The packages we'll need can be installed with the following code: <br>
Note the comment lines, which can be used in R code to tell the interpreter to not read the text as code. They will will tell you a bit more about what you are installing! :)

```{r}
# tidyverse, a versatile and large collection of packages to manipulate data in R
# It provides read and write functions, a more modern dataframe object called tibble
# and the pipe operator %>% allowing you to chain functions together.
# it also contains ggplot2, which we'll use for data visualization.
install.packages("tidyverse")
install.packages("RColorBrewer")

# BioConductor is a package that allows you to install packages from the
# project/repository of the same name, see more in chapter 1.3
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.18")
# Software we'll need for downstream analysis.
BiocManager::install(c(
  "limma", 
  "EnhancedVolcano",
  "ComplexHeatmap",
  "fgsea"
))
```


<div>

<!-- ###################################################################### -->
## Introduction
<!-- ###################################################################### -->
<div style="text-align: justify;">
Becaue of the ever-growing number of publicly available and large datasets in life sciences, computational and statistical skills are becoming a key part of the life scientist's curriculum. This tutorial aims to give you an overview on how to load and convert data in R and how to do exploratory analysis, prior to delving deeper into downstream analysis of differential expression and gene set enrichments.

This tutorial uses the backbone of Dr. Jean-Fred Fontaine's R data analysis tutorial, taught in out other modules.

<div> 

While microarrays are still in use and can be especially useful when interested in a specific set of genes (Custom Arrays), RNA-seq is the main sourch for expression data nowadays. Nevertheless, many techniques, use for RNA-seq count data, in this course are directly - or with minor changes - applicabkle to microarray data too. 

The basic workflow for RNA-seq and computational analysis can be seen in the figures below: 
<div style="display: flex;">
<div style="flex: 80%;">
<figure>
  <img src="images/rna-seq-lab.jpg" alt="Rouch scheme of a wet-lab workflow for RNA-seq.">
  <figcaption>Rouch scheme of a wet-lab workflow for RNA-seq. RNA is isolated from the source, fragmented and subsequently converted with the help of reverse transcriptase to cDNA. From here standard massively parallel sequencing techniques can be used: Adapters are added to the DNA strands, which are used internally in the sequencer and can differ depending on the sequencing chemistry used. The result will be provided in the form of a FASTQ file, that contains millions of short sequences, termed "fragments" or "reads". The color code shows to which steps the illustration on the right roughly belongs. Note that RNA extraction and selection are critical points in the process, that can alter the results significantly, by changing the input amounts and kind of RNA. Adapted from: <https://microbenotes.com/rna-sequencing-principle-steps-types-uses/> by Johannes Wolter</figcaption>
</figure>
</div>

<div style="flex: 20%;">
![](images/rna-seq-lab-2.jpg)
</div>
</div>

<figure>
  <img src="images/expression-analysis-pipelines.png" alt="Caption for my figure">
  <figcaption>Quality control ensures, that the wet-lab experiments, worked as intended and can give the analyist insights on how to handle the data. If there are problematic samples, they can be removed to stop them from biasing the results, certain types of artifacts, namnely batch effects, can also sometimes be detected during this phase and taken into account downstream. The subsequent mapping step holds much information about the quality of the data and should be considered if compute is not too demanding. <br>Trimming can be used to increase the coverage of reads on the reference genome and to filter out faulty reads, increasing the overall quality and useability of a sample. For RNA-seq it is often not as important as for other functional genmoics assays, though. <br>The mapping step is one of the most important in the pipeline. The reads saved in the FASTQ file are mapped, or aligned, to a reference genome. That means, an algorithm searches the whole genome for the best fit of the sequenc of the given read. This step is computationally demanding as millions of reads need to be aligned to a sequence of millions of nucleotides. The resulting BAM files can be annotated and the reads that fall on the locus of a gene be counted, resulting in a gene count matrix. We will use such a matrix, for which the preprocessing was already done in this course. <br>Source: https://biocorecrg.github.io/RNAseq_course_2019/alignment.html</figcaption>
</figure>

<br>

<div style="border: 1px solid black; padding: 10px; background-color: #f9f9f9;">
<p style="text-align: center; font-weight: bold;">Automated Quality Control with ML</p>
If you are interested in the quality control of FASTQ files, inRNA-seq as well as other functional genmoics data, have a look at our work predicting sample quality with machine learning: <br>
Quality prediction: <br> https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02294-2 <br>
Batch effect detection with the same software: <br> https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-04775-y
</div>

<!-- ###################################################################### -->
## Overview
<!-- ###################################################################### -->
<div style="text-align: justify;">

The R language comes with many functions already working, such as arithmetic and logical operators, built-in functions like **dim(), str(), min(), max(), mean()** and much more. As a programming language it also features control structures like loops and allows you to write your own functions. 
It is an object-oriented languae that allows, which allows to pack information from multiple sources into one object, which is often utilized as such by the bioinformatics packages and which we will also encounter. <br


### RLang, packages and Libraries

The base language of R was meant for statistical programming and data visualization, but has been expanded by its community with packages for multiple scietific and programmatic purposes. Especially, for data science, machine learning and bioinformatics. Most packages that handle bioinformatics are available through BioConductor a project dedicated to building and maintaining bioinformatic software: 
<blockquote style="color: #336699; font-family: Arial, sans-serif; font-style: italic; font-size: 18px;">
The mission of the Bioconductor project is to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assays. We are dedicated to building a diverse, collaborative, and welcoming community of developers and data scientists.
</blockquote>
<br> 
To use the packages mentioned above, we'll install them first. You should have tried it already in the prerequisites section. 
If it worked out, we can load the packages - or *libraries* - by calling the **library()** functioon: 
```{r, message=FALSE, warning=FALSE, echo=TRUE, results='hide'}
library(tidyverse)
library(DESeq2) # differential anlysis
library(stats) # standard R pkg for statistics, we'll use its PCA implementation

```

<!-- ###################################################################### -->
<!-- ###################################################################### -->
# Import and Data Objects
<!-- ###################################################################### -->
<!-- ###################################################################### -->


<!-- ###################################################################### -->
## The Dataset
<!-- ###################################################################### -->
Top run any analysis, we first need our data of course. 
We'll get our data from an already prepossessed and curated dataset source called BARRA:CuRDa - weird name I know - but we'll not take the complete set but only choose some sets. The database, with it's downloadable datasets and the paper can be found here: https://sbcb.inf.ufrgs.br/barracurda.
The authors of the database and paper, have used the following workflow, you'll notice some overlaps with what was discussed in the **Introduction**. <br>
<figure>
  <img src="images/barra-curda-workflow.png" alt="Caption for my figure">
  <figcaption>Preprocessing workflow for our datasets.</figcaption>
</figure>

<br>
The dataset was picked and curated to be optimal for machine learning and benchmarked for mutliple algortihms. We'll explore some of that in the second week of the course and compare our differential genes with what the ML algos deem as important in our data. First, we'll focus on exploratory and differential analysis though. 
<br> 
We'll start out with the Head and Neck Cancer datasets **GSEGSE64912** and **GSEGSE68799**.



<!-- ###################################################################### -->
## Importing data into R
<!-- ###################################################################### -->
To demarcate the content of each cell in a file, people use special characters like commas, tabs, pipes, etc. The extension of plain-text files is often a good hint of what delimiter has been used. For example, csv files should use “commas” as the delimiter, whereas tsv files should use “tabs”. Nevertheless, you can find csv or tsv files that are delimited with different characters. 

The first step in data import is locating the file in your computer or in a remote location (e.g. by URL). The most important parameter for readr’s functions is the path to the file of interest.

In this part, you are going to import files located on your machine that you either downloaded from the link above or froim your course files. The **tidyverse** library must be loaded.

### Where are we???
Before actually importing the data: <br>
We need to check where our current **working directory** is and if our files are in a relative path or not. 
If it is not the case we can either choose to include their whole path or set up our working directory in such a way, that we can work from the dedicated folder for the project and have our data and output in the same directory as subdirs. <br> 
When we set up our working directory, we can download the data files and deposit them in the data/input folder.
<span style="color: #336699; font-weight: bold;">Note: the path "C:/Some/random/path/" is an example, you'll not be able to just execute the code below :)</span>
```{r}
# returns the current working dir
getwd()

# sets the current working dir to its input
setwd()
# needs input, otherwise throws an error! :)

# file.path() is a helper function, that glues toigether multiple input strings to a file path. 
# to set a completely new path:
# either
setwd(file.path("C:", "Some", "random", "path"))
# or 
setwd("C:/SOme/random/path")

# set a path relative to the current working dir
setwd(file.path(getwd(), "Some", "downstream", "path"))
```
<br> 

### Excersise 1.1 - Set up your environment
To set up your working dir, you can either work in your respective OS and create a folder called "Bioinfo-module" and set this as the wording dir with the *setwd()* function. 
```{r}
path_dir <- "C:/Some/random/path/bioinfo-module"
 
setwd()
```
Or you can create it from RStudio directly, with the *dir.create()* function.
```{r}
dir.create("bioinfo-module")
setwd(file.path(getwd(), "bioinfo-module"))
```

Now we can set up our input and output directories, which we'll use to have a bit of structure in our working directory.
```{r}
# assign variables to names of out folders (these could also be whole paths, pointing somewhere else entirely!)
out.dir <- "output"
in.dir <- "data"

# create the dirs
dir.create(out.dir)
dir.create(in.dir)
```
<span style="color: #336699; font-weight: bold;">Now you need to deposit the data in the dedicated folder. <br>Note that these files are zipped and need to be unzipped in their destination folder (smaller files can be read in zipped, however files of these size take their time and I don't want to risk it on the NatFak machines :D).</span>

### Importing a csv file
First, we'll import a single file
```{r message=FALSE, warning=FALSE}
test <- read_csv(file.path(in.dir, "GSE68799.csv"))
test
```

Well that worked out well! Note how simple the syntax for data import is: you choose the name of a variable to store your table, use the **<-** operator (written with 2 characters: **<** and **-**), call the appropriate **readr** function and specify the location of your file.
<br>So now we'll get all files in the input folder and read in each of them.
<br> additionally, we'll save the names of the respective files, so that we can keep that info for later. 
```{r, echo=TRUE, results='hide'}
# get the file names, note that this will only return the names of the file not the whole paths
# (whioch we coould do though! Check out the functions vignette, by pressing F1, when the curser is on its text.)
csv_files <- list.files(path = file.path(in.dir), pattern = "\\.csv$")

# now we could write a for loop, to iterate over the list and get each file.
# We'd need to declare a list-object before that has the right size and
# than could put each table into the given list.
# However, loops are not very efficient (especially in R).
# the todyverse has a solution for us: 
# the purrr package provides the map() function which allows us to vectorize operations.

data_list <- map(file.path(in.dir, csv_files), read_csv)
# assign names to list
names(data_list) <- str_remove_all(csv_files, ".csv") 
# loop over list and add a column containin the dataset name!
# we'll again use the a function instead ov writing a loop :)
# this time it's imap(), which also passes the name of a given element not only its contents.
data_list <- imap(data_list, ~ .x %>% mutate(dataset=.y))

# now we can put them together!

combined_data <- bind_rows(data_list)

```


### Exercise 1.2 - Inspect your data
Now that we have our data at hand, we'll need to observe it. 
Use the **head()**, **str()**, **summary()** and **dim()** functions to get an overview of the data. 
Does anything strike you as unexpected?
Try to also inspect other objects with the **str()** and **summary()** function: What kind of object are the paths to you in- and output-folders? What classes of other objects have we assigned toi variables until now? 
```{r}
# str(combined_data)

dim(combined_data)

head(combined_data)

```
What we have at our hands was formated to be used with ML tools: <br>
It shows the variables (features) in the columns and the Observations (samples) in the rows (<span style="color: #336699; font-weight: bold;">samples x features</span>). 
This data format is particularly useful for plotting and most machine learning software, will also expect this format. 
However, in most other applications in R, like statistical functions and also the bioinformtic packages we'll be using, the oposite is expected: 
<span style="color: #336699; font-weight: bold;">features x samples</span>


### Exercise 1.3 - Transpose and build a metadata table metadata

a. Assign a new variable called *meta.data* that hold the general information about the samples. The meta.data should be a dataframe, not a tibble and it should have the ID assigned as **row.names()**. 
b. Assign a new variable called *counts* that hold the numerical information about the samples. It should be Genenames x sample names, the genenames should be assigned as row names. 

Use the **select()** function for both tasks. 
<br>Tipp: you can *deselect* a column, by using **-** infront of the column name.
<br>Tipp: Use **as.data.frame()** and **column_to_rownames()** to accomplish this. 
<br>Feel free to use the pipeoperator **%>%** to select the data. 

```{r,echo=TRUE, results='hide'}
# build metadata
meta.data <- combined_data %>% 
  select(ID, class, dataset) %>% 
  column_to_rownames("ID") %>% 
  as.data.frame()
  
meta.data

# make count matrix
counts <- combined_data %>%
  select(-class, -dataset) %>% 
  column_to_rownames("ID") %>% 
  t()
counts

```

Now that we have transposed our data, we can use the **summary()** function, to get an overview at how the counts are distributed over each sample. 
```{r}
summary(counts)
``` 

<!-- ###################################################################### -->
<!-- ###################################################################### -->
# Exploratory Analysis
<!-- ###################################################################### -->
<!-- ###################################################################### -->


<!-- ###################################################################### -->
## Foreword
<!-- ###################################################################### -->

Exploratory analysis is a critical first step in the interpretation of RNA-seq data, and when working with similar data organized in abundance/count/intensity matrices. This process is akin to an initial survey of the data landscape, aimed at understanding its basic characteristics, identifying potential outliers, and uncovering early patterns or trends. It can also include checking for batch effects, which are technical variations that can introduce unwanted noise into the data. In this respect it can also be used as the "last line" of quality control. 

Visualization tools, such as heatmaps, box plots, and scatter plots, are commonly used in this stage to provide a graphical summary of the data. These can help researchers gain insights into the expression levels of different genes, the similarity between samples, and the overall structure of the data.

In summary, exploratory analysis of RNA-seq count data serves as a foundational step in the data analysis pipeline, setting the stage for more complex downstream analyses, such as differential expression analysis and pathway enrichment analysis. It helps ensure that the data is of high quality and that any subsequent conclusions drawn from the data are reliable and robust.


<!-- ###################################################################### -->
## Principal Component Analysis and Scatter plots. 

We'll first get an overview of the data by plotting the first two components of a PCA. 
This will give us an overview of how similar the samples are to each other and if we have any batch effects to expect from the differing datasets. 

### What's a PCA though?
A Principal Component Analysis, short PCA, is a (linear) *dimension reduction* technique and therefore can be counted to classical unsupervised machine learning. 
It's been around since 1901 and basically computes linear combinations that represent the variance in the data best. To do that it starts, with the longest axis in the data, by minmizing the distance of each point in the data to the first linear combination it computes: The first principal component (PC). This GIF gives a good intuition whats happening under the hood: <br>
<figure>
  <img src="images/first-pc.gif" alt="Caption for my figure">
  <figcaption>Minimization of distances to a line in 2D space.</figcaption>
</figure>
The process is repeated incrementally, but every following PC must be orthogonal to all its predecessors, to ensure the PCs do not overlap in information about the underlying variance of the data. The image below shows three PCs fitted to an ellipsoid. 
<figure>
  <img src="images/3D-pcs.png" alt="Caption for my figure">
  <figcaption>Three PCs fitted to an elipsoid in 3D space, notice how the first PC is in parallel to the longest axis of the ellipsoid and the others are orthogonal to it and each other.</figcaption>
</figure>


<br>The thread I found these images in, is from crossvalidated, a sub-site of stackexchange, where one can often find answers to questions regarding statistics and machine learning. I recommend to check out the thread, it has a number of answers, ranging from very mathematical to much more intuitive explanations (as hopefully given above :D)
<br> Link: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues


### And a scatterplot? 
A scatterplot is the most simple plot you can come up with, it plots two numerical variables against each other leading to a 2D point cloud. We'll plot the first two principal components against each other, as we'll see in a minute. 


### Exercise 2.1 - computing Principal Components and use them for Exploratory Analysis 
a. Load the stats package and use the **prcomp()** function to compute the principal components of our expression data and save it in a variable.
b. Assign the PCs to a new variable by calling the PCA-object you just saved, now add the metadata to it with the **left_join()** function.

Tipp: as we mentioned PCA is a ML technique and most of the time the respective functions will expect the data to be <span style="color: #336699; font-weight: bold;">features x samples</span>, use **t()** to easily transpose the counts object.<br>
Tipp: there is also **rownames_to_column()**
```{r message=FALSE, warning=FALSE, include=T}
# compute PCs
pca_obj <- prcomp(t(counts)) 
df_plot <- pca_obj$x %>% as_tibble(rownames="ID")


# add metadata
df_plot <- meta.data %>% 
  rownames_to_column("ID") %>%
  left_join(df_plot, by = "ID")

```

### Exercise 2.2 - Plotting the information together with metadata.

Now we'll want to plot the first two principal components as a scatterplot. 
For this we'll need functiuons from ggplot2, which is part of the tidyverse. 

ggplot2 has its own pipe-operator to chain together functions that manipulate the final plot, which is **+**. . 

To illustrate how ggplot works, we'll plot the density of the first gene in our count Matrix. For this we'll use our original *combined_data* variable, as its format allows us to include metadata without joining. We'll load a library called **scales** that allows us to easily change the color of fill or color of our plots.
```{r message=FALSE, warning=FALSE, include=T}
# we'll need this for easy different color scales :)
library(scales)

# Filter for the gene of interest
plt <- combined_data %>% 
  select(ID, class, dataset, ENSG00000000003) %>%
  ggplot(aes(x = ENSG00000000003, fill = class, color = class)) +
  geom_density(alpha = 0.5) +
  labs(title = paste("Distribution of reads for", "ENSG00000000003"),
       x = "Count",
       y = "Density",
       fill = "Sample",
       color = "Sample") +
  theme_bw()

# as you saw, plots can also be saved as objects, they can be called to show them or saved with ggsave()
plt
```
Funny that we directly see a clear difference, that was actually not planned! 
Let's have a look what changes, when we change either fill or color to *dataset* instead of *class*. <br>You can add ```scale_fill_brewer(palette = "Dark2") + 
scale_color_brewer(palette = "Set1")``` to change the colors of fill and color respectively.
```{r message=FALSE, warning=FALSE, include=F}
# we'll need this for easy different color scales :)
library(scales)

# Filter for the gene of interest
plt <- combined_data %>% 
  select(ID, class, dataset, ENSG00000000003) %>%
  ggplot(aes(x = ENSG00000000003, fill = dataset, color = class)) +
  geom_density(alpha = 0.5) +
  labs(title = paste("Distribution of reads for", "ENSG00000000003"),
       x = "Count",
       y = "Density",
       fill = "Sample",
       color = "Sample") +
    scale_fill_brewer(palette = "Dark2") +  # for fill color
  scale_color_brewer(palette = "Set1") +  # for border color
  theme_bw()

# as you saw, plots can also be saved as objects, they can be called to show them or saved with ggsave()
plt
```

Now use what you saw above with your dataframe containing the PCs and metadata. Use the **geom_point()** function to plot a scatterplot!
Save the plot with the **ggsave()** function in the *out.dir*. 
```{r include=T}
# Exercise 2.2
plt <- df_plot %>% 
  ...

plt


# suggested filename: "PCA_head-and-neck.png"
ggsave(...)

```

This is how you plot should look like: 

```{r, echo=F}
# solution
plt <- df_plot %>% 
  ggplot(aes(x=PC1, y=PC2, color=class, shape=dataset)) +
      geom_point() 

plt
# ggsave(filename = file.path(out.dir, 'PCA_head-and-neck.png'), plot=plt)
```
What you can see, if you made it work, is a classic *Batch effect*! The main variance in the dataset (first PC) is explained by the difference ebtween the dataset.
Feel free to experiment a bit and check how the PCA scatter plot looks like for the respective datasets alone!

<div style="border: 1px solid black; padding: 10px; background-color: #f9f9f9;">
<p style="text-align: center; font-weight: bold;">Batch effects and other artifacts</p>
Artifacts is a word for underlying factors that effect the data we are working with, but are not associated with the biological observation. By distorting the gene counts or introducing biases of other nature, they will impact the downstream analysis and lead to false results. The most abundant and impactful are the **batch effects**, which stem from technical differences in the experimental phase of an assy: Different Handlers, reagents, machines are just a few sources for them. Sometimes they correlate well with differences in quality of a sample, as we showed in one of the works of the Automated Quality Control box. We'll see how to handle them in later sections.
</div>

### Excersise 2.3 - Screeplots and percentage of Variance explained.
Principal components can tell us a lot about the data, the more you need to adequately descripe its variance, the more diverse it is. It is intersting for us to see how much variance is actually covered by the PCs. First we'll get the standard deviations, which are stored in our PCA object inspect the object to see in which part of it they could be stored. Don't forget that we'll need to square the standard deviation to get the variance!

```{r}
# first we'll get the standard deviations, which are stored in our PCA object
# inspect the object to see in which part of it they could be stored.
# don't forget that we'll need to square the standard deviation to get the variance!
# Then we Create a data frame for the scree plot
scree_data <- data.frame(
  PC = paste("PC", 1:length(pca_obj$...)),
  Variance =  pca_obj$... ...
)
# we'll just take the first 20 
scree_data <- scree_data[...,]

# now we can create a scree plot which basically plots the PCs in their order
# and the respective variance explained by them. 
plt <- ggplot(scree_data, aes(x = reorder(PC, Variance, decreasing=T), y = Variance)) +
  geom_bar(stat = "identity") +
  labs(title = "Scree Plot",
       x = "Principal Component",
       y = "Variance Explained") +
  theme_bw()
plt

```

```{r, echo=F}
# soolution:
# first we'll get the standard deviations, which are stored in our PCA object
# Create a data frame for the scree plot
scree_data <- data.frame(
  PC = paste("PC", 1:length(pca_obj$sdev)),
  Variance =  pca_obj$sdev^2
)
# we'll just take the first 20 
scree_data <- scree_data[1:20,]
# now we can create a scree plot which basically plots the PCs in their order
# and the repective variuance expleined by them. 
plt <- ggplot(scree_data, aes(x = reorder(PC, Variance, decreasing=T), y = Variance)) +
  geom_bar(stat = "identity") +
  labs(title = "Scree Plot",
       x = "Principal Component",
       y = "Variance Explained") +
  theme_bw()
plt

```
Additionally we can compute the percentage of variance that each of the PCs explains as follows:
```{r}
#hint: perhaps you already encountered these numbers when inspecting the pca_obj with summary?
percentage <- round(df_pca$sdev^2 / sum(df_pca$sdev^2) * 100, 2)
percentage

```
So the batch effect we observed on the PCA plot quite drastically is actually generating **42%** of the Variance in out data!

## Heatmaps and clustering

<!-- perhaps later with the differential genes? -->
# Differential Expression Analysis

## Foreword

Differential expression analysis is a key step in interpreting RNA-seq data, aiming to identify genes whose expression levels differ significantly between conditions. This process can reveal important insights into biological processes and diseases.

Classical statistical methods, such as t-tests, may not work well for RNA-seq data due to its unique characteristics. RNA-seq data is typically overdispersed, meaning the variance exceeds the mean, violating the assumptions of many classical statistical methods. Additionally, RNA-seq data is count-based and often has a large number of zero counts, which can lead to false positives or negatives when using classical methods.

To address these challenges, specialized statistical methods and tools have been developed for differential expression analysis of RNA-seq data, such as DESeq2, edgeR, and limma-voom. These methods account for the overdispersion and count-based nature of RNA-seq data, providing more accurate and reliable results.

The process typically involves data normalization, dispersion estimation, and hypothesis testing. The result is a list of genes ranked by significance, along with statistics such as fold change and false discovery rate (FDR).

In summary, differential expression analysis is a powerful tool for identifying genes that are differentially expressed between conditions, providing valuable insights into the underlying biology. However, it's important to use appropriate statistical methods and tools that account for the unique characteristics of RNA-seq data.

## Limma 

Limma (Linear Models for Microarray Data) is a widely used R package for the analysis of gene expression data, including both microarray and RNA-seq data. Limma was originally developed for the analysis of microarray data, but has since been adapted for the analysis of RNA-seq data using the voom transformation.

The voom (variance modeling at the observation level) transformation is used to transform RNA-seq count data into a format that can be analyzed using linear models. The voom transformation estimates the mean-variance relationship of the count data and generates a set of weights that are used to account for the variability in the data. These weights are then used in the linear modeling step to improve the accuracy and power of the analysis. We will not use voom, as our data already has been normalized with a method that works similar (DESeq2's VST).

Limma uses linear models to test for differential expression between conditions, while accounting for various sources of variability, such as batch effects and other covariates. The linear models include design matrices that specify the experimental design and allow for the estimation of log2 fold changes between conditions.

One of the key features of limma is its empirical Bayes method for estimating gene-wise variances. This method borrows information across genes to improve the stability and accuracy of the variance estimates, especially for genes with low counts. 

Reference: 
Ritchie ME, Phipson B, Wu D, Hu Y, Law CW, Shi W, Smyth GK (2015). “limma powers differential expression analyses for RNA-sequencing and microarray studies.” Nucleic Acids Research, 43(7), e47. doi:10.1093/nar/gkv007. 

### Exercise 3.1 - Differential expression on a single dataset
Lets start with the basic analysis of one of our datasets!
We'll see how to handle batch effects in the next chapter and then use both datasets. We'll settle for GSE68799, as the difference between Normal and Tumor samples was much clearer in the PCA, making it likely the "easier" dataset.
```{r}
# Exercise 3.1.0: get subset of dataset with the select() function and the metadata table
# Tipp: get the row names of the metadata and subset it for the correct dataset prior.
input_cts <- counts[,...]
```

```{r, echo=F}
# solution

# Exercise 3.1.0: get subset of dataset with the select() function and the metadata table
input_cts <- counts[,meta.data %>% filter(dataset=="GSE68799") %>% rownames()]
```


Now on to the implementation of a standard limma workflow. 
Feel free to use the internet, there are a plethora of tutorials online. Remember, that you DO NOT need to use voom as we already have normalized data.
```{r}
# Load the limma package
library(limma)

# subset the metadata.
new.meta <- meta.data %>% 
  filter(dataset=="GSE68799") %>% 
  mutate(class=as.factor(class)) %>% 
  mutate(dataset=as.factor(dataset))

# Exercise 3.1.1: Create a limma object. You'll need a model.matrix,
# to function as design, you can use the information in the mew.mate dataframe
design <- model.matrix(...)

# Exercise 3.1.2 Create a fit object and subsequently call th ebayes function to estimate gene-wise variance
fit <- ...
fit <- ...

# Exercise 3.1.3 Get the results with the topTable function
res <- topTable(...)

# Exercise 3.1.4. Filter the results based on significance (e.g., adj.P.Val < 0.05)
# tipp, try the subset function, or use filter()
res_sig <- ...


```

```{r, echo=F}
# solution
# Load the limma package
library(limma)

# subset the metadata.
new.meta <- meta.data %>% 
  filter(dataset=="GSE68799") %>% 
  mutate(class=as.factor(class)) %>% 
  mutate(dataset=as.factor(dataset))

# Exercise 3.1.1: Create a limma object. You'll need a model.matrix,
# to function as design, you can use the information in the mew.mate dataframe
design <- model.matrix(...)

# Exercise 3.1.2 Create a fit object and subsequently call th ebayes function to estimate gene-wise variance
# fits linear model
fit <- lmFit(input_cts, design)
# estimates gene-wise variance and corrects residues of LM
fit <- eBayes(fit)

# Exercise 3.1.3 Get the results with the
res <- topTable(fit, number = Inf,  coef = 2)

# Filter the results based on significance (e.g., adj.P.Val < 0.05)
# tipp, try the subset function, or use filter()
res_sig <- res %>% filter(adj.P.Val < 0.05)
# or
res_sig <- subset(res, adj.P.Val < 0.05)

```

Now we do have a number of differentially expressed genes, which we can plot, subset and further categorize. 

### Exercise 3.2 P-values and Co
Why do we use the adjusted P-value? 
<br>Which adjustment method uses limma?

ToDO: Box about P-values

