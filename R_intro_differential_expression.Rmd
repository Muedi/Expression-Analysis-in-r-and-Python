---
title: "Expression analysis with R and the tidyverse"
output: 
  html_notebook:
    theme: readable
    highlight: tango
    number_sections: true
    toc: true
    toc_depth: 2
    toc_float: true
---
<style>
#header .btn-group {
    display: none;
}
</style>

<!-- TO DO: GIVE MORE EXPLANATIONS ABOUT WORKING DIRECTORY -->
<!-- TO DO: RENAME EXERCISES BY A, B, C, ... -->

<!-- ###################################################################### -->
<!-- ###################################################################### -->
# Intro and Overview
<!-- ###################################################################### -->
<!-- ###################################################################### -->

<!-- ###################################################################### -->
## Pre-requisites
<!-- ###################################################################### -->

### R and RStudio
<div style="text-align: justify;">
Make sure that **R** and **RStudio** are installed in your computer. You can create a new **R Script** in **RStudio** to copy example source code from this tutorial and write code for the exercises. Save the script in your home directory (or a sub directory of your choice). To run the code written on the current line (or of selected lines), click on button **Run** or press **CTRL+ENTER**. Results will be displayed either in the **RStudio**'s console or the Viewer panel (graphics). To read the documentation about a function (Help panel), in the script, place the cursor on the function's name and press F1. You should use this feature extensively here, as many answers to possible questions are hidden here :)


### package installation
The packages we'll need can be installed with the following code: <br>
Note the comment lines, which can be used in R code to tell the interpreter to not read the text as code. They will will tell you a bit more about what you are installing! :)

```{r}
# tidyverse, a versatile and large collection of packages to manipulate data in R
# It provides read and write functions, a more modern dataframe object called tibble
# and the pipe operator %>% allowing you to chain functions together.
# it also contains ggplot2, which we'll use for data visualization.
install.packages("tidyverse")
install.packages("RColorBrewer")

# BioConductor is a package that allows you to install packages from the
# project/repository of the same name, see more in chapter 1.3
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")
BiocManager::install(version = "3.18")
# Software we'll need for downstream analysis.
BiocManager::install(c(
  "DESeq2",
  "limma", 
  "edgeR",
  "EnhancedVolcano",
  "ComplexHeatmap",
  "fgsea"
))
```


<div>

<!-- ###################################################################### -->
## Introduction
<!-- ###################################################################### -->
<div style="text-align: justify;">
Becaue of the ever-growing number of publicly available and large datasets in life sciences, computational and statistical skills are becoming a key part of the life scientist's curriculum. This tutorial aims to give you an overview on how to load and convert data in R and how to do exploratory analysis, prior to delving deeper into downstream analysis of differential expression and gene set enrichments.

This tutorial uses the backbone of Dr. Jean-Fred Fontaine's R data analysis tutorial, taught in out other modules.

<div> 

While microarrays are still in use and can be especially useful when interested in a specific set of genes (Custom Arrays), RNA-seq is the main sourch for expression data nowadays. Nevertheless, many techniques, use for RNA-seq count data, in this course are directly - or with minor changes - applicabkle to microarray data too. 

The basic workflow for RNA-seq and computational analysis can be seen in the figures below: 

<div style="display: flex;">
<div style="flex: 80%;">
![Rouch scheme of a wet-lab workflow for RNA-seq. RNA is isolated from the source, fragmented and subsequently converted with the help of reverse transcriptse to CDNA. From here standard massively parallel sequencing techniques can be used: Adapters are added to the DNA strands, which are used internally in the sequencer and can differ dependeing on the sequencing chemistry used. The resszult, will be provided in form of a FASTQ file, that contains millions of short sequences, termed "fragments "or "reads". \nThe color code, shows to which steps the illustration on the right roughly belong. Note that RNA-extraction and selection are critical points in the process, that can alter the results significantly, by changeing the input amounts and kind of RNA. <br> Adapted from: https://microbenotes.com/rna-sequencing-principle-steps-types-uses/ by Johannes Wolter](images/rna-seq-lab.jpg)
</div> 

<div style="flex: 20%;">
![](images/rna-seq-lab-2.jpg)
</div>
</div>

![Quality control ensures, that the wet-lab experiments, worked as intended and can give the analyist insights on how to handle the data. If there are problematic samples, they can be removed to stop them from biasing the results, certain types of artifacts, namnely batch effects, can also sometimes be detected during this phase and taken into account downstream. The subsequent mapping step holds much information about the quality of the data and should be considered if compute is not too demanding. <br>Trimming can be used to increase the coverage of reads on the reference genome and to filter out faulty reads, increasing the overall quality and useability of a sample. For RNA-seq it is often not as important as for other functional genmoics assays, though. <br>The mapping step is one of the most important in the pipeline. The reads saved in the FASTQ file are mapped, or aligned, to a reference genome. That means, an algorithm searches the whole genome for the best fit of the sequenc of the given read. This step is computationally demanding as millions of reads need to be aligned to a sequence of millions of nucleotides. The resulting BAM files can be annotated and the reads that fall on the locus of a gene be counted, resulting in a gene count matrix. We will use such a matrix, for which the preprocessing was already done in this course. <br>Source: https://biocorecrg.github.io/RNAseq_course_2019/alignment.html ](images/expression-analysis-pipelines.png)

<br>

<div style="border: 1px solid black; padding: 10px; background-color: #f9f9f9;">
<p style="text-align: center; font-weight: bold;">Automated Quality Control with ML</p>
If you are interested in the quality control of FASTQ files, inRNA-seq as well as other functional genmoics data, have a look at our work predicting sample quality with machine learning: <br>
Quality prediction: <br> https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02294-2 <br>
Batch effect detection with the same software: <br> https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-022-04775-y
</div>

<!-- ###################################################################### -->
## Overview
<!-- ###################################################################### -->
<div style="text-align: justify;">

The R language comes with many functions already working, such as arithmetic and logical operators, built-in functions like **dim(), str(), min(), max(), mean()** and much more. As a programming language it also features control structures like loops and allows you to write your own functions. 
It is an object-oriented languae that allows, which allows to pack information from multiple sources into one object, which is often utilized as such by the bioinformatics packages and which we will also encounter. <br


### RLang, packages and Libraries

The base language of R was meant for statistical programming and data visualization, but has been expanded by its community with packages for multiple scietific and programmatic purposes. Especially, for data science, machine learning and bioinformatics. Most packages that handle bioinformatics are available through BioConductor a project dedicated to building and maintaining bioinformatic software: 
<blockquote style="color: #336699; font-family: Arial, sans-serif; font-style: italic; font-size: 18px;">
The mission of the Bioconductor project is to develop, support, and disseminate free open source software that facilitates rigorous and reproducible analysis of data from current and emerging biological assays. We are dedicated to building a diverse, collaborative, and welcoming community of developers and data scientists.
</blockquote>
<br> 
To use the packages mentioned above, we'll install them first. You should have tried it already in the prerequisites section. 
If it worked out, we can load the packages - or *libraries* - by calling the **library()** functioon: 
```{r, message=FALSE, warning=FALSE, echo=TRUE, results='hide'}
library(tidyverse)
library(DESeq2) # differential anlysis
library(stats) # standard R pkg for statistics, we'll use its PCA implementation

```

<!-- ###################################################################### -->
<!-- ###################################################################### -->
# Import and data objects
<!-- ###################################################################### -->
<!-- ###################################################################### -->


<!-- ###################################################################### -->
## The Dataset
<!-- ###################################################################### -->
Top run any analysis, we first need our data of course. 
We'll get our data from an already prepossessed and curated dataset source called BARRA:CuRDa - weird name I know - but we'll not take the complete set but only choose some sets. The database, with it's downloadable datasets and the paper can be found here: https://sbcb.inf.ufrgs.br/barracurda.
The authors of the database and paper, have used the following workflow, you'll notice some overlaps with what was discussed in the **Introduction**. <br>
![](images/barra-curda-workflow.png)
<br>
The dataset was picked and curated to be optimal for machine learning and benchmarked for mutliple algortihms. We'll explore some of that in the second week of the course and compare our differential genes with what the ML algos deem as important in our data. First, we'll focus on exploratory and differential analysis though. 
<br> 
We'll start out with the Head and Neck Cancer datasets **GSEGSE64912** and **GSEGSE68799**.



<!-- ###################################################################### -->
## Importing data into R
<!-- ###################################################################### -->
To demarcate the content of each cell in a file, people use special characters like commas, tabs, pipes, etc. The extension of plain-text files is often a good hint of what delimiter has been used. For example, csv files should use “commas” as the delimiter, whereas tsv files should use “tabs”. Nevertheless, you can find csv or tsv files that are delimited with different characters. 

The first step in data import is locating the file in your computer or in a remote location (e.g. by URL). The most important parameter for readr’s functions is the path to the file of interest.

In this part, you are going to import files located on your machine that you either downloaded from the link above or froim your course files. The **tidyverse** library must be loaded.

### Where are we???
Before actually importing the data: <br>
We need to check where our current **working directory** is and if our files are in a relative path or not. 
If it is not the case we can either choose to include their whole path or set up our working directory in such a way, that we can work from the dedicated folder for the project and have our data and output in the same directory as subdirs. <br> 
When we set up our working directory, we can download the data files and deposit them in the data/input folder.
<span style="color: #336699; font-weight: bold;">Note: the path "C:/Some/random/path/" is an example, you'll not be able to just execute the code below :)</span>
```{r}
# returns the current working dir
getwd()

# sets the current working dir to its input
setwd()
# needs input, otherwise throws an error! :)

# file.path() is a helper function, that glues toigether multiple input strings to a file path. 
# to set a completely new path:
# either
setwd(file.path("C:", "Some", "random", "path"))
# or 
setwd("C:/SOme/random/path")

# set a path relative to the current working dir
setwd(file.path(getwd(), "Some", "downstream", "path"))
```
<br> 

### Excersise 1. 
To set up your working dir, you can either work in your respective OS and create a folder called "Bioinfo-module" and set this as the wording dir with the *setwd()* function. 
```{r}
path_dir <- "C:/Some/random/path/bioinfo-module"
 
setwd()
```
Or you can create it from RStudio directly, with the *dir.create()* function.
```{r}
dir.create("bioinfo-module")
setwd(file.path(getwd(), "bioinfo-module"))
```

Now we can set up our input and output directories, which we'll use to have a bit of structure in our working directory.
```{r}
# assign variables to names of out folders (these could also be whole paths, pointing somewhere else entirely!)
out.dir <- "output"
in.dir <- "data"

# create the dirs
dir.create(out.dir)
dir.create(in.dir)
```
<span style="color: #336699; font-weight: bold;">Now you need to deposit the data in the dedicated folder. <br>Note that these files are zipped and need to be unzipped in their destination folder (smaller files can be read in zipped, however files of these size take their time and I don't want to risk it on the NatFak machines :D).</span>

### Importing a csv file
First, we'll import a single file
```{r message=FALSE, warning=FALSE}
test <- read_csv(file.path(in.dir, "GSE68799.csv"))
test
```

Well that worked out well! Note how simple the syntax for data import is: you choose the name of a variable to store your table, use the **<-** operator (written with 2 characters: **<** and **-**), call the appropriate **readr** function and specify the location of your file.
<br>So now we'll get all files in the input folder and read in each of them.
<br> additionally, we'll save the names of the respective files, so that we can keep that info for later. 
```{r, echo=TRUE, results='hide'}
# get the file names, note that this will only return the names of the file not the whole paths
# (whioch we coould do though! Check out the functions vignette, by pressing F1, when the curser is on its text.)
csv_files <- list.files(path = file.path(in.dir), pattern = "\\.csv$")

# now we could write a for loop, to iterate over the list and get each file.
# We'd need to declare a list-object before that has the right size and
# than could put each table into the given list.
# However, loops are not very efficient (especially in R).
# the todyverse has a solution for us: 
# the purrr package provides the map() function which allows us to vectorize operations.

data_list <- map(file.path(in.dir, csv_files), read_csv)
# assign names to list
names(data_list) <- str_remove_all(csv_files, ".csv") 
# loop over list and add a column containin the dataset name!
# we'll again use the a function instead ov writing a loop :)
# this time it's imap(), which also passes the name of a given element not only its contents.
data_list <- imap(data_list, ~ .x %>% mutate(dataset=.y))

# now we can put them together!

combined_data <- bind_rows(data_list)

```


### Exercise 1.2 <br>Inspect your data
Now that we have our data at hand, we'll need to observe it. 
Use the **head()**, **str()** and **dim()** functions to get an overview of the data. 
Does anything strike you as unexpected?
try to also inspect other objects with the **str()** and **summary** function: What kind of object are the paths to you in- and output-folders? What classes of other objects have we assigned toi variables until now? 
```{r}
# str(combined_data)

dim(combined_data)

head(combined_data)

```
What we have at our hands was formated to be used with ML tools: <br>
It shows the variables (features) in the columns and the Observations (samples) in the rows (<span style="color: #336699; font-weight: bold;">samples x features</span>). 
This data format is particularly useful for plotting and most machine learning software, will also expect this format. 
However, in most other applications in R, like statistical functions and also the bioinformtic packages we'll be using, the oposite is expected: 
<span style="color: #336699; font-weight: bold;">features x samples</span>


### Exercise 1.3 <br>Transpose and build a metadata table metadata

a. Assign a new variable called *meta.data* that hold the general information about the samples. The meta.data should be a dataframe, not a tibble and it should have the ID assigned as **row.names()**. 
b. Assign a new variable called *counts* that hold the numerical information about the samples. It should be Genenames x sample names, the genenames should be assigned as row names. 

Use the **select()** function for both tasks. 
<br>Tipp: you can *deselect* a column, by using **-** infront of the column name.
<br>Tipp: Use **as.data.frame()** and **column_to_rownames()** to accomplish this. 
<br>Feel free to use the pipeoperator **%>%** to select the data. 

```{r,echo=TRUE, results='hide'}
# build metadata
meta.data <- combined_data %>% 
  select(ID, class, dataset) %>% 
  column_to_rownames("ID") %>% 
  as.data.frame()
  
meta.data

# make count matrix
counts <- combined_data %>%
  select(-class, -dataset) %>% 
  column_to_rownames("ID") %>% 
  t()
counts

```

Now that we have transposed our data, we can use the **summary()** function, to get an overview at how the counts are distributed over each sample. 
```{r}
summary(counts)
``` 

<!-- ###################################################################### -->
<!-- ###################################################################### -->
# Exploratory Analysis
<!-- ###################################################################### -->
<!-- ###################################################################### -->


<!-- ###################################################################### -->
## Foreword
<!-- ###################################################################### -->

Exploratory analysis is a critical first step in the interpretation of RNA-seq data, and when working with similar data organized in abundance/count/intensity matrices. This process is akin to an initial survey of the data landscape, aimed at understanding its basic characteristics, identifying potential outliers, and uncovering early patterns or trends. It can also include checking for batch effects, which are technical variations that can introduce unwanted noise into the data. In this respect it can also be used as the "last line" of quality control. 

Visualization tools, such as heatmaps, box plots, and scatter plots, are commonly used in this stage to provide a graphical summary of the data. These can help researchers gain insights into the expression levels of different genes, the similarity between samples, and the overall structure of the data.

In summary, exploratory analysis of RNA-seq count data serves as a foundational step in the data analysis pipeline, setting the stage for more complex downstream analyses, such as differential expression analysis and pathway enrichment analysis. It helps ensure that the data is of high quality and that any subsequent conclusions drawn from the data are reliable and robust.


<!-- ###################################################################### -->
## Principal Component Analysis and Scatter plots. 

We'll first get an overview of the data by plotting the first two components of a PCA. 
This will give us an overview of how similar the samples are to each other and if we have any batch effects to expect from the differing datasets. 

### What's a PCA though?
A Principal Component Analysis, short PCA, is a (linear) *dimension reduction* technique and therefore can be counted to classical unsupervised machine learning. 
It's been around since 1901 and basically computes linear combinations that represent the variance in the data best. To do that it starts, with the longest axis in the data, by minmizing the distance of each point in the data to the first linear combination it computes: The first principal component (PC). This GIF gives a good intuition whats happening under the hood: <br>
![](images/first-pc.gif)
The process is repeated incrementally, but every following PC must be orthogonal to all its predecessors, to ensure the PCs do not overlap in information about the underlying variance of the data. The image below shows three PCs fitted to an ellipsoid. 

![](images/3D-pcs.png)

The thread I found these images is from crossvalidated, a sub-site of stackexchange, where one can often find answers to questions regarding statistics and machine learning. I recommend to check out the thread, it has a number of answers, ranging from very mathematical to much more intuitive explanations (as hopefully given above :D)
<br> Link: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues


### And a scatterplot? 
A scatterplot is the most simple plot you can come up with, it plots two numerical variables against each other leading to a 2D point cloud. We'll plot the first two principal components against each other, as we'll see in a minute. 


### Exercise 2.1 <br> computing PCA
a. Load the stats package and use the **prcomp()** function to compute the principal components of our expression data and save it in a variable.
b. Assign the PCs to a new variable by calling the PCA-object you just saved, now add the metadata to it with the **left_join()** function.

Tipp: as we mentioned PCA is a ML technique and most of the time the respective functions will expect the data to be <span style="color: #336699; font-weight: bold;">features x samples</span>, use **t()** to easily transpose the counts object.<br>
Tipp: there is also **rownames_to_column()**
```{r message=FALSE, warning=FALSE, include=T}
# compute PCs
pca_obj <- prcomp(t(counts)) 
df_plot <- pca_obj$x %>% as_tibble(rownames="ID")


# add metadata
df_plot <- meta.data %>% 
  rownames_to_column("ID") %>%
  left_join(df_plot, by = "ID")

```

### Exercise 2.2 <br>plotting the information together with metadata.

Now we'll want to plot the first two principal components as a scatterplot. 
For this we'll need functiuons from ggplot2, which is part of the tidyverse. 

ggplot2 has its own pipe-operator to chain together functions that manipulate the final plot, which is **+**. . 

To illustrate how ggplot works, we'll plot the density of the first gene in our count Matrix. For this we'll use our original *combined_data* variable, as its format allows us to include metadata without joining. We'll load a library called **scales** that allows us to easily change the color of fill or color of our plots.
```{r message=FALSE, warning=FALSE, include=T}
# we'll need this for easy different color scales :)
library(scales)

# Filter for the gene of interest
plt <- combined_data %>% 
  select(ID, class, dataset, ENSG00000000003) %>%
  ggplot(aes(x = ENSG00000000003, fill = class, color = class)) +
  geom_density(alpha = 0.5) +
  labs(title = paste("Distribution of reads for", "ENSG00000000003"),
       x = "Count",
       y = "Density",
       fill = "Sample",
       color = "Sample") +
  theme_bw()

# as you saw, plots can also be saved as objects, they can be called to show them or saved with ggsave()
plt
```
Funny that we directly see a clear difference, that was actually not planned! 
Let's have a look what changes, when we change either fill or color to *dataset* instead of *class*. <br>You can add ```scale_fill_brewer(palette = "Dark2") + 
scale_color_brewer(palette = "Set1")``` to change the colors of fill and color respectively.
```{r message=FALSE, warning=FALSE, include=F}
# we'll need this for easy different color scales :)
library(scales)

# Filter for the gene of interest
plt <- combined_data %>% 
  select(ID, class, dataset, ENSG00000000003) %>%
  ggplot(aes(x = ENSG00000000003, fill = dataset, color = class)) +
  geom_density(alpha = 0.5) +
  labs(title = paste("Distribution of reads for", "ENSG00000000003"),
       x = "Count",
       y = "Density",
       fill = "Sample",
       color = "Sample") +
    scale_fill_brewer(palette = "Dark2") +  # for fill color
  scale_color_brewer(palette = "Set1") +  # for border color
  theme_bw()

# as you saw, plots can also be saved as objects, they can be called to show them or saved with ggsave()
plt
```

Now use what you saw above with your dataframe containing the PCs and metadata.
Save the plot with the ggsave function. 
```{r include=T}

plt <- df_plot %>% 
  ggplot(aes(x=PC1, y=PC2, color=class, shape=dataset)) +
      geom_point() 

plt
# ggsave(filename = file.path(out.dir, 'PCA_head-and-neck.png'), plot=plt)
```
What you can see, if you made it work, is a classic *Batch effect*! The main variance in the dataset (first PC) is explained by the difference ebtween the dataset.
Feel free to experiment a bit and check how the PCA scatter plot looks like for the respective datasets alone!

<div style="border: 1px solid black; padding: 10px; background-color: #f9f9f9;">
<p style="text-align: center; font-weight: bold;">Batch effects and other artifacts</p>
Artifacts is a word for underlying factors that effect the data we are working with, but are not associated with the biological observation. By distorting the gene counts or introducing biases of other nature, they will impact the downstream analysis and lead to false results. The most abundant and impactful are the **batch effects**, which stem from technical differences in the experimental phase of an assy: Different Handlers, reagents, machines are just a few sources for them. Sometimes they correlate well with differences in quality of a sample, as we showed in one of the works of the Automated Quality Control box. We'll see how to handle them in later sections.
</div>

### Excersise 3 - Screeplots and percentage of Variance explained.
Principal components can tell us a lot about the data, the more you need to adequately descripe its variance, the more diverse it is. It is intersting for us to see how much variance is actually covered by the PCs. 

```{r}
# first we'll get the standard deviations, which are stored in our PCA object
# Create a data frame for the scree plot
scree_data <- data.frame(
  PC = paste("PC", 1:length(pca_obj$sdev)),
  Variance =  pca_obj$sdev^2
)
# we'll just take the first 20 
scree_data <- scree_data[1:20,]
# now we can create a scree plot which basically plots the PCs in their order
# and the repective variuance expleined by them. 
plt <- ggplot(scree_data, aes(x = reorder(PC, Variance, decreasing=T), y = Variance)) +
  geom_bar(stat = "identity") +
  labs(title = "Scree Plot",
       x = "Principal Component",
       y = "Variance Explained") +
  theme_bw()
plt

```
Additionally we can compute the percentage of variance that each of the PCs explains as follows:
```{r}
percentage <- round(df_pca$sdev^2 / sum(df_pca$sdev^2) * 100, 2)
percentage
```
So the batch effect we observed on the PCA plot quite drastically is actually generating **42%** of the Variance in out data!

## Heatmaps and clustering

## Differential expression

